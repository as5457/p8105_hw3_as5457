---
title: "p8105_hw3_as5457"
author: "Sunny Siddique"
date: "October 8, 2018"
output: github_document
---

Setting global settings for the format of the graphs generated for this homework. 
```{r}

#Loading the tidyverse package. 
library(tidyverse)

#Ensuring that the generated graphs are of reasonable height and width.
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

#Ensuring that the background of the graphs are bw and legend is printed at the bottom. 
theme_set(theme_bw() + theme(legend.position = "right"))
```

------------------------------------------------------------------------------------------------

#Problem 1

**Load the BRFSS data**

First, do some data cleaning:

- Format the data to use appropriate variable names;
- Focus on the “Overall Health” topic
- Include only responses from “Excellent” to “Poor”
- Organize responses as a factor taking levels from “Excellent” to “Poor”

```{r}
#Loading the dataset from the p8105.datasets package
library(p8105.datasets)
data("brfss_smart2010")

#Defining a new dataset called brfss_smart from the original dataset
brfss_smart = brfss_smart2010 %>% 
  
#Cleaning variable names
janitor::clean_names(dat = .) %>% 
  
#Keeping only rows where the topic was "Overall Health"
  filter(topic == "Overall Health") %>% 
  
#Including only responses from "Excellent" to "Poor"
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  
#Organizing responses as a factor taking levels from "Excellent" to "Poor"
  mutate(response = forcats::fct_reorder(response, display_order))

#Checking if conversion of the response variable from character to factor worked
class(brfss_smart$response)
```


**Using this dataset, do or answer the following (commenting on the results of each):**

- In 2002, which states were observed at 7 locations?

*The states that were observed at 7 locations were CT, FL, NC.*

```{r}
brfss_smart %>%
  
#Filtering to only keep data for 2002
  filter(year == 2002) %>% 
  
#Separating locationdesc to be able to count distinct state and county
  separate(locationdesc, into = c("state", "county"), sep = " - ") %>% 
  distinct(state, county) %>% 
  
#Counting number of states represented
  count(state) %>% 
  
#Filtering states that were observed 7 times
  filter (n == 7)
```


- Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r}
#Making spaghetting plot called "Spaghetti"
spaghetti = brfss_smart %>% 
  
#Grouping by the year and state abbreviation variables.
  group_by (year, locationabbr) %>% 
  
#Counting the number of times the state variable is observed
  summarize (n = n()) %>% 
  
#Setting up ggplot to show the spaghetti plot with points and lines connected.
  ggplot(aes(x = year, y = n, color = locationabbr)) +
  geom_point() +
  geom_line() + 
  labs (
    title = "Number of Observations in Each State by Year",
        x = "year",
        y = "number of observations"
  )

#Viewing the spaghetti plot
spaghetti
```


- Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.


```{r}

#Filtering to only keep data for the years 2002, 2006 and 2010
year02 = brfss_smart %>% 
  filter (year == 2002)
year06 = brfss_smart %>%
  filter (year == 2006)
year10 = brfss_smart %>% 
  filter (year == 2010)

#Combining the year-filtered datasets into one dataset
filteredyear = bind_rows(year02, year06, year10)

#Continuing to filter variables
filteredyear %>% 
  
#Filtering data for NY
  filter(locationabbr == "NY") %>% 
  
#Filtering data for those who responded "Excellent"
  filter(response == "Excellent") %>% 
  
#Grouping by year variable
  group_by(year) %>% 
  
#Calculating mean and standard deviation
  summarize(mean_prop = mean(data_value),
            std_prop = sd(data_value))
```

- For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
#First I am converting the year variable from "integer" to "Factor" to be able to make my boxplot show each year.
brfss_smart$year=as.factor(brfss_smart$year)
```


```{r}
brfss_smart %>% 
  
#Grouping by the response, locationabbr, year variables
  group_by (response, locationabbr, year) %>% 
  
#Calculating the mean for each response value.
  summarize(avg_prop = mean (data_value)) %>% 
  
#Setting up boxplot using ggplot.
  ggplot(aes(x = year, y = avg_prop)) + 
  geom_boxplot(alpha = .5) +
  facet_grid(. ~ response) +
  labs (
    title = "Distribution of State-Level Averages Over Time",
        x = "Year",
        y = "Average Proportion"
  ) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



# Problem 2
Importing the instacart data

```{r}
#Loading the dataset from the p8105.datasets package
library(p8105.datasets)
data("instacart")

#Defining a new dataset called brfss_smart from the original dataset
insta = instacart %>% 
  
#Cleaning variable names
janitor::clean_names(dat = .)
```

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.

The instacart dataset provides information regarding each individual order that was placed and the products that were ordered. Additionally, it contains information regarding where in the supermarket (aisle) the ordered products can be found. The dataset size is `r dim_desc(insta)` with rows containing repeated informmation regarding each order and columns showing important variables such as the Order ID, Product Name, Time of the Order, Location and Department of the Order etc. There are `r nrow(distinct(insta, user_id))` distinct customers who collectively ordered `r nrow(distinct(insta, product_id))` orders.



**How many aisles are there, and which aisles are the most items ordered from?**

*There are 134 aisles.*

```{r}
unique_aisle = unique(insta$aisle_id)
length(unique_aisle)
```

*The aisles with the most items ordered are: fresh vegetables, fresh fruits, packaged vegetables & fruits, yogurt and packaged cheese.*
```{r}
insta %>%
  group_by(aisle) %>% 
  summarize(item_count = n()) %>% 
  arrange(desc(item_count))
```


Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
insta %>%
  group_by(aisle, department) %>% 
  summarize(item_count = n()) %>% 
  ggplot(aes(x = aisle, y = item_count, color = department)) + 
  geom_point(aes(size = item_count), alpha = 1) +
  labs (
    title = "Number Of Items Ordered In Each Aisle",
        x = "Aisle Name",
        y = "Number Of Items Ordered"
  ) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
insta %>% 
  group_by(aisle, product_name) %>% 
  summarize (item_counts = n()) %>% 
  mutate(rank = min_rank(desc(item_counts))) %>% 
  filter (rank == 1) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"))
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
insta %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour_day = mean(order_hour_of_day)) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  spread(key = "order_dow", value = "mean_hour_day") %>% 
  knitr::kable()
```

#Problem 3

This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package (it’s called ny_noaa).

```{r}
library(p8105.datasets)
data("ny_noaa")
weather = ny_noaa %>% 
  janitor::clean_names(dat = .)
```

*The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.*

The NY NOAA dataset provides information regarding the weather and precipitation pattern from `r min(weather$date)` to `r max(weather$date)`. The size of the dataset is `r dim_desc(weather)` and data are structured with columns providing information on precipitation (rain/snow) and daily temperatures (tmin/tmax). These key variables allow us to analyze the weather patterns in NY over the past decade. There are `r sum(is.na(weather$prcp))` missing values for the precipitation colummn and `r sum(is.na(weather$tmax))` missing values for tmax and `r sum(is.na(weather$tmin))` missing values for tmin. Since a large extent of the data are missing, this might be an issue in the analysis of this dataset because we would only be able to restrict our analysis to the specific stations for which data are available. 

*Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.*

```{r}
ny_weather <- weather %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(prcp = prcp / 10) %>% 
  mutate(tmax = as.numeric(tmax) / 10) %>% 
  mutate(tmin = as.numeric(tmin)/10)
```

*For snowfall, what are the most commonly observed values? Why?*

The most common values observed for snowfall are 0 and null. This is because most days of the year it does not snow and many stations do not collect information on snow. 
```{r}
ny_weather %>% 
  count(snow) %>% 
  arrange(desc(n))
```

*Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?*

Observing the collection of box plots below, we can see that in general, the average tmax in January is significantly lower in January than in July (as expected). The median of the mean tmax in January mostly falls beetween -5C and +5C whereas in July the values approximately range between 25C and 30C. There are many outliers in the January plot on both ends of the boxplots. In the July plot, however, the outliers are all in the lower tail of the boxplots, meaning that in July tmax can be low for some stations compared to what is expected.

```{r}
ny_weather %>% 
  filter (month %in% c("01", "07")) %>%
  group_by(month, id, year) %>% 
  summarize(average_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = average_tmax)) +
  geom_boxplot() +
  facet_grid(~month) +
    labs (
    title = "Average Maximum Temperature In January and July",
        x = "Year",
        y = "Average Maximum Temperature (C)"
  ) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


